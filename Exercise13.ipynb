{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEkpmV25UWXx"
   },
   "source": [
    "# Exercise 13 \n",
    "\n",
    "## CIFAR-10 classification using Keras Conv2D\n",
    "\n",
    "The CIFAR-10 small photo classification problem is a standard dataset used in computer vision and deep learning.\n",
    "\n",
    "\n",
    "\n",
    "## CIFAR-10 Photo Classification Dataset\n",
    "\n",
    "CIFAR comes from the Canadian Institute For Advanced Research and the CIFAR-10 dataset was developed along with the CIFAR-100 dataset by researchers at the CIFAR institute. The dataset is comprised of 60,000 32Ã—32 pixel color photographs of objects from 10 classes with following class labels:\n",
    "0. airplane\n",
    "1. automobile\n",
    "2. bird\n",
    "3. cat\n",
    "4. deer\n",
    "5. dog\n",
    "6. frog\n",
    "7. horse\n",
    "8. ship\n",
    "9. truck\n",
    "\n",
    "It is relatively straightforward to achieve 80% classification accuracy. Top performance on the problem is achieved by deep learning convolutional neural networks with a classification accuracy above 90% on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PvJ88p5hJFS2",
    "outputId": "ffcda2e7-b54d-4ed4-f051-da5f2597fb61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T1EERkR3DTxN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuzvVAwgDrHE",
    "outputId": "01b3dda8-0b06-4838-fa9b-daa8c1265d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "781/781 [==============================] - 32s 37ms/step - loss: 2.1346 - accuracy: 0.2942 - val_loss: 1.5455 - val_accuracy: 0.4401\n",
      "Epoch 2/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 1.6204 - accuracy: 0.4075 - val_loss: 1.5100 - val_accuracy: 0.4503\n",
      "Epoch 3/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 1.5040 - accuracy: 0.4492 - val_loss: 1.5536 - val_accuracy: 0.4404\n",
      "Epoch 4/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.4278 - accuracy: 0.4783 - val_loss: 1.5925 - val_accuracy: 0.4326\n",
      "Epoch 5/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.3567 - accuracy: 0.5057 - val_loss: 1.4686 - val_accuracy: 0.4726\n",
      "Epoch 6/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.3071 - accuracy: 0.5278 - val_loss: 1.3604 - val_accuracy: 0.5123\n",
      "Epoch 7/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.2629 - accuracy: 0.5417 - val_loss: 1.3739 - val_accuracy: 0.5060\n",
      "Epoch 8/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.2262 - accuracy: 0.5592 - val_loss: 1.3159 - val_accuracy: 0.5332\n",
      "Epoch 9/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.1855 - accuracy: 0.5747 - val_loss: 1.1878 - val_accuracy: 0.5770\n",
      "Epoch 10/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 1.1542 - accuracy: 0.5899 - val_loss: 1.2987 - val_accuracy: 0.5462\n",
      "Epoch 11/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.1239 - accuracy: 0.5968 - val_loss: 1.1079 - val_accuracy: 0.6025\n",
      "Epoch 12/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 1.1014 - accuracy: 0.6070 - val_loss: 1.2077 - val_accuracy: 0.5706\n",
      "Epoch 13/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.0747 - accuracy: 0.6177 - val_loss: 1.0159 - val_accuracy: 0.6290\n",
      "Epoch 14/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 1.0484 - accuracy: 0.6293 - val_loss: 1.0059 - val_accuracy: 0.6401\n",
      "Epoch 15/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.0277 - accuracy: 0.6348 - val_loss: 0.9948 - val_accuracy: 0.6415\n",
      "Epoch 16/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 1.0031 - accuracy: 0.6415 - val_loss: 1.1561 - val_accuracy: 0.5936\n",
      "Epoch 17/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.9848 - accuracy: 0.6517 - val_loss: 0.9497 - val_accuracy: 0.6606\n",
      "Epoch 18/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.9686 - accuracy: 0.6578 - val_loss: 0.9542 - val_accuracy: 0.6634\n",
      "Epoch 19/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.9499 - accuracy: 0.6643 - val_loss: 0.9049 - val_accuracy: 0.6768\n",
      "Epoch 20/400\n",
      "781/781 [==============================] - 30s 39ms/step - loss: 0.9395 - accuracy: 0.6703 - val_loss: 0.8876 - val_accuracy: 0.6837\n",
      "Epoch 21/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.9190 - accuracy: 0.6768 - val_loss: 0.9414 - val_accuracy: 0.6637\n",
      "Epoch 22/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.9104 - accuracy: 0.6804 - val_loss: 0.8607 - val_accuracy: 0.6955\n",
      "Epoch 23/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8944 - accuracy: 0.6847 - val_loss: 0.8622 - val_accuracy: 0.6954\n",
      "Epoch 24/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8881 - accuracy: 0.6886 - val_loss: 0.8465 - val_accuracy: 0.6987\n",
      "Epoch 25/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.8741 - accuracy: 0.6956 - val_loss: 0.7842 - val_accuracy: 0.7193\n",
      "Epoch 26/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8629 - accuracy: 0.6971 - val_loss: 0.8206 - val_accuracy: 0.7080\n",
      "Epoch 27/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.8633 - accuracy: 0.6985 - val_loss: 0.8569 - val_accuracy: 0.6999\n",
      "Epoch 28/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8441 - accuracy: 0.7024 - val_loss: 0.9741 - val_accuracy: 0.6553\n",
      "Epoch 29/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.8375 - accuracy: 0.7091 - val_loss: 0.8302 - val_accuracy: 0.7068\n",
      "Epoch 30/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.8319 - accuracy: 0.7100 - val_loss: 0.7693 - val_accuracy: 0.7294\n",
      "Epoch 31/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8245 - accuracy: 0.7099 - val_loss: 0.8151 - val_accuracy: 0.7112\n",
      "Epoch 32/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.8123 - accuracy: 0.7155 - val_loss: 0.7813 - val_accuracy: 0.7257\n",
      "Epoch 33/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8067 - accuracy: 0.7200 - val_loss: 0.8157 - val_accuracy: 0.7184\n",
      "Epoch 34/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.7983 - accuracy: 0.7187 - val_loss: 0.8446 - val_accuracy: 0.7038\n",
      "Epoch 35/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7916 - accuracy: 0.7230 - val_loss: 0.8057 - val_accuracy: 0.7163\n",
      "Epoch 36/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7866 - accuracy: 0.7243 - val_loss: 0.8253 - val_accuracy: 0.7145\n",
      "Epoch 37/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.7770 - accuracy: 0.7293 - val_loss: 0.7338 - val_accuracy: 0.7402\n",
      "Epoch 38/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7676 - accuracy: 0.7323 - val_loss: 0.6854 - val_accuracy: 0.7581\n",
      "Epoch 39/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7678 - accuracy: 0.7335 - val_loss: 0.7298 - val_accuracy: 0.7468\n",
      "Epoch 40/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.7585 - accuracy: 0.7354 - val_loss: 0.7647 - val_accuracy: 0.7306\n",
      "Epoch 41/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.7510 - accuracy: 0.7369 - val_loss: 0.7478 - val_accuracy: 0.7375\n",
      "Epoch 42/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.7481 - accuracy: 0.7388 - val_loss: 0.7383 - val_accuracy: 0.7418\n",
      "Epoch 43/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7436 - accuracy: 0.7417 - val_loss: 0.6954 - val_accuracy: 0.7592\n",
      "Epoch 44/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7355 - accuracy: 0.7437 - val_loss: 0.6772 - val_accuracy: 0.7619\n",
      "Epoch 45/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.7235 - accuracy: 0.7478 - val_loss: 0.7351 - val_accuracy: 0.7454\n",
      "Epoch 46/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7251 - accuracy: 0.7483 - val_loss: 0.6890 - val_accuracy: 0.7609\n",
      "Epoch 47/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.7209 - accuracy: 0.7499 - val_loss: 0.6955 - val_accuracy: 0.7598\n",
      "Epoch 48/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7080 - accuracy: 0.7529 - val_loss: 0.6711 - val_accuracy: 0.7652\n",
      "Epoch 49/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.7102 - accuracy: 0.7530 - val_loss: 0.6995 - val_accuracy: 0.7568\n",
      "Epoch 50/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7010 - accuracy: 0.7576 - val_loss: 0.7066 - val_accuracy: 0.7587\n",
      "Epoch 51/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6921 - accuracy: 0.7606 - val_loss: 0.6195 - val_accuracy: 0.7849\n",
      "Epoch 52/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.6862 - accuracy: 0.7628 - val_loss: 0.6290 - val_accuracy: 0.7811\n",
      "Epoch 53/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6862 - accuracy: 0.7613 - val_loss: 0.6419 - val_accuracy: 0.7800\n",
      "Epoch 54/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.6815 - accuracy: 0.7629 - val_loss: 0.6442 - val_accuracy: 0.7780\n",
      "Epoch 55/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6838 - accuracy: 0.7631 - val_loss: 0.6717 - val_accuracy: 0.7655\n",
      "Epoch 56/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6699 - accuracy: 0.7685 - val_loss: 0.6784 - val_accuracy: 0.7673\n",
      "Epoch 57/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6659 - accuracy: 0.7705 - val_loss: 0.6849 - val_accuracy: 0.7648\n",
      "Epoch 58/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6652 - accuracy: 0.7692 - val_loss: 0.6385 - val_accuracy: 0.7821\n",
      "Epoch 59/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.6578 - accuracy: 0.7718 - val_loss: 0.6353 - val_accuracy: 0.7823\n",
      "Epoch 60/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6515 - accuracy: 0.7756 - val_loss: 0.6434 - val_accuracy: 0.7801\n",
      "Epoch 61/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6499 - accuracy: 0.7747 - val_loss: 0.6132 - val_accuracy: 0.7924\n",
      "Epoch 62/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6505 - accuracy: 0.7769 - val_loss: 0.6646 - val_accuracy: 0.7760\n",
      "Epoch 63/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.6423 - accuracy: 0.7806 - val_loss: 0.5639 - val_accuracy: 0.8068\n",
      "Epoch 64/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6414 - accuracy: 0.7794 - val_loss: 0.6380 - val_accuracy: 0.7836\n",
      "Epoch 65/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6351 - accuracy: 0.7811 - val_loss: 0.5896 - val_accuracy: 0.7984\n",
      "Epoch 66/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6342 - accuracy: 0.7796 - val_loss: 0.6388 - val_accuracy: 0.7824\n",
      "Epoch 67/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6307 - accuracy: 0.7829 - val_loss: 0.5963 - val_accuracy: 0.7949\n",
      "Epoch 68/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6221 - accuracy: 0.7855 - val_loss: 0.5973 - val_accuracy: 0.7963\n",
      "Epoch 69/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6157 - accuracy: 0.7869 - val_loss: 0.6169 - val_accuracy: 0.7921\n",
      "Epoch 70/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6146 - accuracy: 0.7881 - val_loss: 0.5889 - val_accuracy: 0.7993\n",
      "Epoch 71/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.6137 - accuracy: 0.7893 - val_loss: 0.6111 - val_accuracy: 0.7941\n",
      "Epoch 72/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.6093 - accuracy: 0.7902 - val_loss: 0.6208 - val_accuracy: 0.7893\n",
      "Epoch 73/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6112 - accuracy: 0.7905 - val_loss: 0.5554 - val_accuracy: 0.8093\n",
      "Epoch 74/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6038 - accuracy: 0.7907 - val_loss: 0.6179 - val_accuracy: 0.7914\n",
      "Epoch 75/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5985 - accuracy: 0.7961 - val_loss: 0.5153 - val_accuracy: 0.8250\n",
      "Epoch 76/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5985 - accuracy: 0.7942 - val_loss: 0.5501 - val_accuracy: 0.8135\n",
      "Epoch 77/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5956 - accuracy: 0.7949 - val_loss: 0.5587 - val_accuracy: 0.8102\n",
      "Epoch 78/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5947 - accuracy: 0.7956 - val_loss: 0.6248 - val_accuracy: 0.7903\n",
      "Epoch 79/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5887 - accuracy: 0.7986 - val_loss: 0.5663 - val_accuracy: 0.8086\n",
      "Epoch 80/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5835 - accuracy: 0.8007 - val_loss: 0.5487 - val_accuracy: 0.8132\n",
      "Epoch 81/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5786 - accuracy: 0.8019 - val_loss: 0.5918 - val_accuracy: 0.8027\n",
      "Epoch 82/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.5810 - accuracy: 0.7988 - val_loss: 0.5415 - val_accuracy: 0.8152\n",
      "Epoch 83/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5783 - accuracy: 0.8009 - val_loss: 0.5501 - val_accuracy: 0.8135\n",
      "Epoch 84/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5740 - accuracy: 0.8042 - val_loss: 0.5825 - val_accuracy: 0.8008\n",
      "Epoch 85/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5739 - accuracy: 0.8030 - val_loss: 0.5570 - val_accuracy: 0.8100\n",
      "Epoch 86/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5662 - accuracy: 0.8053 - val_loss: 0.5702 - val_accuracy: 0.8078\n",
      "Epoch 87/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5696 - accuracy: 0.8054 - val_loss: 0.5126 - val_accuracy: 0.8278\n",
      "Epoch 88/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5620 - accuracy: 0.8083 - val_loss: 0.5465 - val_accuracy: 0.8131\n",
      "Epoch 89/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5590 - accuracy: 0.8088 - val_loss: 0.5364 - val_accuracy: 0.8172\n",
      "Epoch 90/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5614 - accuracy: 0.8064 - val_loss: 0.5450 - val_accuracy: 0.8163\n",
      "Epoch 91/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5609 - accuracy: 0.8083 - val_loss: 0.5339 - val_accuracy: 0.8200\n",
      "Epoch 92/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5512 - accuracy: 0.8098 - val_loss: 0.5242 - val_accuracy: 0.8235\n",
      "Epoch 93/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5480 - accuracy: 0.8119 - val_loss: 0.5366 - val_accuracy: 0.8187\n",
      "Epoch 94/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5492 - accuracy: 0.8133 - val_loss: 0.5090 - val_accuracy: 0.8314\n",
      "Epoch 95/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5452 - accuracy: 0.8118 - val_loss: 0.4942 - val_accuracy: 0.8301\n",
      "Epoch 96/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5405 - accuracy: 0.8158 - val_loss: 0.5562 - val_accuracy: 0.8123\n",
      "Epoch 97/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5420 - accuracy: 0.8154 - val_loss: 0.5449 - val_accuracy: 0.8171\n",
      "Epoch 98/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5362 - accuracy: 0.8165 - val_loss: 0.5461 - val_accuracy: 0.8166\n",
      "Epoch 99/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5335 - accuracy: 0.8152 - val_loss: 0.5509 - val_accuracy: 0.8155\n",
      "Epoch 100/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5351 - accuracy: 0.8167 - val_loss: 0.5538 - val_accuracy: 0.8100\n",
      "Epoch 101/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5360 - accuracy: 0.8166 - val_loss: 0.5144 - val_accuracy: 0.8286\n",
      "Epoch 102/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5337 - accuracy: 0.8178 - val_loss: 0.5069 - val_accuracy: 0.8302\n",
      "Epoch 103/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5299 - accuracy: 0.8193 - val_loss: 0.4826 - val_accuracy: 0.8382\n",
      "Epoch 104/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5248 - accuracy: 0.8187 - val_loss: 0.5338 - val_accuracy: 0.8187\n",
      "Epoch 105/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5262 - accuracy: 0.8195 - val_loss: 0.4953 - val_accuracy: 0.8330\n",
      "Epoch 106/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5173 - accuracy: 0.8236 - val_loss: 0.5219 - val_accuracy: 0.8224\n",
      "Epoch 107/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5216 - accuracy: 0.8229 - val_loss: 0.5077 - val_accuracy: 0.8266\n",
      "Epoch 108/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5181 - accuracy: 0.8211 - val_loss: 0.4795 - val_accuracy: 0.8389\n",
      "Epoch 109/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5192 - accuracy: 0.8228 - val_loss: 0.4613 - val_accuracy: 0.8413\n",
      "Epoch 110/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5123 - accuracy: 0.8248 - val_loss: 0.4592 - val_accuracy: 0.8465\n",
      "Epoch 111/400\n",
      "781/781 [==============================] - 27s 35ms/step - loss: 0.5094 - accuracy: 0.8247 - val_loss: 0.4851 - val_accuracy: 0.8375\n",
      "Epoch 112/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5082 - accuracy: 0.8259 - val_loss: 0.4828 - val_accuracy: 0.8373\n",
      "Epoch 113/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5083 - accuracy: 0.8254 - val_loss: 0.5169 - val_accuracy: 0.8279\n",
      "Epoch 114/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5038 - accuracy: 0.8280 - val_loss: 0.5133 - val_accuracy: 0.8280\n",
      "Epoch 115/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5029 - accuracy: 0.8286 - val_loss: 0.4654 - val_accuracy: 0.8444\n",
      "Epoch 116/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.5032 - accuracy: 0.8279 - val_loss: 0.4602 - val_accuracy: 0.8455\n",
      "Epoch 117/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5026 - accuracy: 0.8281 - val_loss: 0.4889 - val_accuracy: 0.8353\n",
      "Epoch 118/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.4971 - accuracy: 0.8294 - val_loss: 0.4838 - val_accuracy: 0.8400\n",
      "Epoch 119/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4964 - accuracy: 0.8305 - val_loss: 0.4742 - val_accuracy: 0.8409\n",
      "Epoch 120/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4940 - accuracy: 0.8299 - val_loss: 0.4918 - val_accuracy: 0.8383\n",
      "Epoch 121/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4893 - accuracy: 0.8318 - val_loss: 0.5136 - val_accuracy: 0.8282\n",
      "Epoch 122/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4904 - accuracy: 0.8338 - val_loss: 0.4679 - val_accuracy: 0.8457\n",
      "Epoch 123/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4887 - accuracy: 0.8340 - val_loss: 0.4535 - val_accuracy: 0.8494\n",
      "Epoch 124/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4891 - accuracy: 0.8332 - val_loss: 0.4548 - val_accuracy: 0.8473\n",
      "Epoch 125/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4871 - accuracy: 0.8335 - val_loss: 0.4752 - val_accuracy: 0.8403\n",
      "Epoch 126/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4913 - accuracy: 0.8317 - val_loss: 0.5088 - val_accuracy: 0.8332\n",
      "Epoch 127/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4821 - accuracy: 0.8344 - val_loss: 0.5058 - val_accuracy: 0.8349\n",
      "Epoch 128/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4843 - accuracy: 0.8354 - val_loss: 0.4366 - val_accuracy: 0.8554\n",
      "Epoch 129/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4843 - accuracy: 0.8345 - val_loss: 0.5015 - val_accuracy: 0.8325\n",
      "Epoch 130/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.4776 - accuracy: 0.8379 - val_loss: 0.4499 - val_accuracy: 0.8498\n",
      "Epoch 131/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4806 - accuracy: 0.8361 - val_loss: 0.4843 - val_accuracy: 0.8405\n",
      "Epoch 132/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4753 - accuracy: 0.8374 - val_loss: 0.4981 - val_accuracy: 0.8393\n",
      "Epoch 133/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4740 - accuracy: 0.8358 - val_loss: 0.4482 - val_accuracy: 0.8510\n",
      "Epoch 134/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4705 - accuracy: 0.8396 - val_loss: 0.4593 - val_accuracy: 0.8489\n",
      "Epoch 135/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.4708 - accuracy: 0.8395 - val_loss: 0.4748 - val_accuracy: 0.8438\n",
      "Epoch 136/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4734 - accuracy: 0.8385 - val_loss: 0.4646 - val_accuracy: 0.8442\n",
      "Epoch 137/400\n",
      "781/781 [==============================] - 28s 35ms/step - loss: 0.4682 - accuracy: 0.8407 - val_loss: 0.4578 - val_accuracy: 0.8504\n",
      "Epoch 138/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4661 - accuracy: 0.8409 - val_loss: 0.4625 - val_accuracy: 0.8473\n",
      "Epoch 139/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4674 - accuracy: 0.8404 - val_loss: 0.4512 - val_accuracy: 0.8519\n",
      "Epoch 140/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4623 - accuracy: 0.8415 - val_loss: 0.5892 - val_accuracy: 0.8108\n",
      "Epoch 141/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4609 - accuracy: 0.8420 - val_loss: 0.4755 - val_accuracy: 0.8419\n",
      "Epoch 142/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4582 - accuracy: 0.8436 - val_loss: 0.4194 - val_accuracy: 0.8615\n",
      "Epoch 143/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4605 - accuracy: 0.8438 - val_loss: 0.4305 - val_accuracy: 0.8582\n",
      "Epoch 144/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4597 - accuracy: 0.8416 - val_loss: 0.4721 - val_accuracy: 0.8446\n",
      "Epoch 145/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4538 - accuracy: 0.8442 - val_loss: 0.4337 - val_accuracy: 0.8566\n",
      "Epoch 146/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4587 - accuracy: 0.8441 - val_loss: 0.4207 - val_accuracy: 0.8601\n",
      "Epoch 147/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4507 - accuracy: 0.8451 - val_loss: 0.4405 - val_accuracy: 0.8535\n",
      "Epoch 148/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4524 - accuracy: 0.8441 - val_loss: 0.4467 - val_accuracy: 0.8529\n",
      "Epoch 149/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4512 - accuracy: 0.8457 - val_loss: 0.4444 - val_accuracy: 0.8525\n",
      "Epoch 150/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4560 - accuracy: 0.8449 - val_loss: 0.4552 - val_accuracy: 0.8520\n",
      "Epoch 151/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4504 - accuracy: 0.8460 - val_loss: 0.4500 - val_accuracy: 0.8516\n",
      "Epoch 152/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4461 - accuracy: 0.8474 - val_loss: 0.4560 - val_accuracy: 0.8511\n",
      "Epoch 153/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4486 - accuracy: 0.8473 - val_loss: 0.5299 - val_accuracy: 0.8316\n",
      "Epoch 154/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4494 - accuracy: 0.8453 - val_loss: 0.4348 - val_accuracy: 0.8574\n",
      "Epoch 155/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4458 - accuracy: 0.8464 - val_loss: 0.4375 - val_accuracy: 0.8563\n",
      "Epoch 156/400\n",
      "781/781 [==============================] - 30s 39ms/step - loss: 0.4414 - accuracy: 0.8511 - val_loss: 0.4161 - val_accuracy: 0.8624\n",
      "Epoch 157/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.4425 - accuracy: 0.8492 - val_loss: 0.4682 - val_accuracy: 0.8463\n",
      "Epoch 158/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4424 - accuracy: 0.8486 - val_loss: 0.4527 - val_accuracy: 0.8509\n",
      "Epoch 159/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4422 - accuracy: 0.8488 - val_loss: 0.4479 - val_accuracy: 0.8517\n",
      "Epoch 160/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4425 - accuracy: 0.8471 - val_loss: 0.4322 - val_accuracy: 0.8604\n",
      "Epoch 161/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4365 - accuracy: 0.8495 - val_loss: 0.4335 - val_accuracy: 0.8564\n",
      "Epoch 162/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4336 - accuracy: 0.8513 - val_loss: 0.4490 - val_accuracy: 0.8525\n",
      "Epoch 163/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4315 - accuracy: 0.8515 - val_loss: 0.4197 - val_accuracy: 0.8637\n",
      "Epoch 164/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4367 - accuracy: 0.8495 - val_loss: 0.4526 - val_accuracy: 0.8517\n",
      "Epoch 165/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4340 - accuracy: 0.8515 - val_loss: 0.4405 - val_accuracy: 0.8574\n",
      "Epoch 166/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4307 - accuracy: 0.8528 - val_loss: 0.4125 - val_accuracy: 0.8642\n",
      "Epoch 167/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.4337 - accuracy: 0.8515 - val_loss: 0.4806 - val_accuracy: 0.8418\n",
      "Epoch 168/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4333 - accuracy: 0.8526 - val_loss: 0.4574 - val_accuracy: 0.8508\n",
      "Epoch 169/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4313 - accuracy: 0.8527 - val_loss: 0.4220 - val_accuracy: 0.8613\n",
      "Epoch 170/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4250 - accuracy: 0.8543 - val_loss: 0.4128 - val_accuracy: 0.8623\n",
      "Epoch 171/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4260 - accuracy: 0.8544 - val_loss: 0.4214 - val_accuracy: 0.8611\n",
      "Epoch 172/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4253 - accuracy: 0.8550 - val_loss: 0.4033 - val_accuracy: 0.8699\n",
      "Epoch 173/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4193 - accuracy: 0.8553 - val_loss: 0.4748 - val_accuracy: 0.8434\n",
      "Epoch 174/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4231 - accuracy: 0.8568 - val_loss: 0.4133 - val_accuracy: 0.8636\n",
      "Epoch 175/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4206 - accuracy: 0.8558 - val_loss: 0.4389 - val_accuracy: 0.8559\n",
      "Epoch 176/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4206 - accuracy: 0.8564 - val_loss: 0.4245 - val_accuracy: 0.8609\n",
      "Epoch 177/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4177 - accuracy: 0.8581 - val_loss: 0.4591 - val_accuracy: 0.8529\n",
      "Epoch 178/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4173 - accuracy: 0.8572 - val_loss: 0.4307 - val_accuracy: 0.8596\n",
      "Epoch 179/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4197 - accuracy: 0.8552 - val_loss: 0.4760 - val_accuracy: 0.8455\n",
      "Epoch 180/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4186 - accuracy: 0.8567 - val_loss: 0.4173 - val_accuracy: 0.8624\n",
      "Epoch 181/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4136 - accuracy: 0.8568 - val_loss: 0.4026 - val_accuracy: 0.8673\n",
      "Epoch 182/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4159 - accuracy: 0.8570 - val_loss: 0.4227 - val_accuracy: 0.8607\n",
      "Epoch 183/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4186 - accuracy: 0.8579 - val_loss: 0.4163 - val_accuracy: 0.8630\n",
      "Epoch 184/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4109 - accuracy: 0.8608 - val_loss: 0.4163 - val_accuracy: 0.8651\n",
      "Epoch 185/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4064 - accuracy: 0.8607 - val_loss: 0.4061 - val_accuracy: 0.8675\n",
      "Epoch 186/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4058 - accuracy: 0.8617 - val_loss: 0.4219 - val_accuracy: 0.8611\n",
      "Epoch 187/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4097 - accuracy: 0.8592 - val_loss: 0.4001 - val_accuracy: 0.8677\n",
      "Epoch 188/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4110 - accuracy: 0.8590 - val_loss: 0.4155 - val_accuracy: 0.8614\n",
      "Epoch 189/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4069 - accuracy: 0.8614 - val_loss: 0.4117 - val_accuracy: 0.8670\n",
      "Epoch 190/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4043 - accuracy: 0.8614 - val_loss: 0.4141 - val_accuracy: 0.8655\n",
      "Epoch 191/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4094 - accuracy: 0.8605 - val_loss: 0.4328 - val_accuracy: 0.8578\n",
      "Epoch 192/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4046 - accuracy: 0.8627 - val_loss: 0.4036 - val_accuracy: 0.8661\n",
      "Epoch 193/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.4047 - accuracy: 0.8620 - val_loss: 0.4102 - val_accuracy: 0.8643\n",
      "Epoch 194/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4055 - accuracy: 0.8609 - val_loss: 0.3896 - val_accuracy: 0.8696\n",
      "Epoch 195/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4019 - accuracy: 0.8635 - val_loss: 0.4459 - val_accuracy: 0.8537\n",
      "Epoch 196/400\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.3984 - accuracy: 0.8642 - val_loss: 0.4182 - val_accuracy: 0.8646\n",
      "Epoch 197/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4021 - accuracy: 0.8628 - val_loss: 0.4147 - val_accuracy: 0.8667\n",
      "Epoch 198/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4044 - accuracy: 0.8617 - val_loss: 0.4293 - val_accuracy: 0.8606\n",
      "Epoch 199/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.3974 - accuracy: 0.8644 - val_loss: 0.4021 - val_accuracy: 0.8694\n",
      "Epoch 200/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3995 - accuracy: 0.8632 - val_loss: 0.4210 - val_accuracy: 0.8608\n",
      "Epoch 201/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3950 - accuracy: 0.8633 - val_loss: 0.4018 - val_accuracy: 0.8677\n",
      "Epoch 202/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3934 - accuracy: 0.8660 - val_loss: 0.4071 - val_accuracy: 0.8657\n",
      "Epoch 203/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3953 - accuracy: 0.8656 - val_loss: 0.3995 - val_accuracy: 0.8705\n",
      "Epoch 204/400\n",
      "781/781 [==============================] - 30s 39ms/step - loss: 0.3916 - accuracy: 0.8659 - val_loss: 0.3934 - val_accuracy: 0.8731\n",
      "Epoch 205/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3895 - accuracy: 0.8658 - val_loss: 0.4031 - val_accuracy: 0.8697\n",
      "Epoch 206/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3940 - accuracy: 0.8655 - val_loss: 0.4157 - val_accuracy: 0.8652\n",
      "Epoch 207/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3926 - accuracy: 0.8655 - val_loss: 0.4146 - val_accuracy: 0.8677\n",
      "Epoch 208/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3907 - accuracy: 0.8651 - val_loss: 0.4189 - val_accuracy: 0.8650\n",
      "Epoch 209/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3874 - accuracy: 0.8668 - val_loss: 0.3952 - val_accuracy: 0.8716\n",
      "Epoch 210/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.3879 - accuracy: 0.8674 - val_loss: 0.3962 - val_accuracy: 0.8721\n",
      "Epoch 211/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3897 - accuracy: 0.8664 - val_loss: 0.4089 - val_accuracy: 0.8695\n",
      "Epoch 212/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3904 - accuracy: 0.8665 - val_loss: 0.4045 - val_accuracy: 0.8660\n",
      "Epoch 213/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3939 - accuracy: 0.8656 - val_loss: 0.3941 - val_accuracy: 0.8725\n",
      "Epoch 214/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3819 - accuracy: 0.8689 - val_loss: 0.4192 - val_accuracy: 0.8663\n",
      "Epoch 215/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3855 - accuracy: 0.8683 - val_loss: 0.3799 - val_accuracy: 0.8771\n",
      "Epoch 216/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.3910 - accuracy: 0.8648 - val_loss: 0.3800 - val_accuracy: 0.8744\n",
      "Epoch 217/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3817 - accuracy: 0.8697 - val_loss: 0.4049 - val_accuracy: 0.8696\n",
      "Epoch 218/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3846 - accuracy: 0.8668 - val_loss: 0.4680 - val_accuracy: 0.8498\n",
      "Epoch 219/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3899 - accuracy: 0.8666 - val_loss: 0.4047 - val_accuracy: 0.8678\n",
      "Epoch 220/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.3849 - accuracy: 0.8682 - val_loss: 0.3925 - val_accuracy: 0.8710\n",
      "Epoch 221/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3835 - accuracy: 0.8673 - val_loss: 0.4253 - val_accuracy: 0.8635\n",
      "Epoch 222/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.3769 - accuracy: 0.8707 - val_loss: 0.3907 - val_accuracy: 0.8722\n",
      "Epoch 223/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3817 - accuracy: 0.8693 - val_loss: 0.3896 - val_accuracy: 0.8724\n",
      "Epoch 224/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3767 - accuracy: 0.8702 - val_loss: 0.4417 - val_accuracy: 0.8605\n",
      "Epoch 225/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3753 - accuracy: 0.8711 - val_loss: 0.4165 - val_accuracy: 0.8664\n",
      "Epoch 226/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3813 - accuracy: 0.8701 - val_loss: 0.4061 - val_accuracy: 0.8688\n",
      "Epoch 227/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3819 - accuracy: 0.8701 - val_loss: 0.4055 - val_accuracy: 0.8686\n",
      "Epoch 228/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3712 - accuracy: 0.8707 - val_loss: 0.4225 - val_accuracy: 0.8649\n",
      "Epoch 229/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3700 - accuracy: 0.8737 - val_loss: 0.3933 - val_accuracy: 0.8723\n",
      "Epoch 230/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3711 - accuracy: 0.8732 - val_loss: 0.3972 - val_accuracy: 0.8735\n",
      "Epoch 231/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3754 - accuracy: 0.8704 - val_loss: 0.3869 - val_accuracy: 0.8726\n",
      "Epoch 232/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3673 - accuracy: 0.8750 - val_loss: 0.3965 - val_accuracy: 0.8702\n",
      "Epoch 233/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3758 - accuracy: 0.8710 - val_loss: 0.3859 - val_accuracy: 0.8731\n",
      "Epoch 234/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3735 - accuracy: 0.8718 - val_loss: 0.3817 - val_accuracy: 0.8767\n",
      "Epoch 235/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3683 - accuracy: 0.8744 - val_loss: 0.3921 - val_accuracy: 0.8711\n",
      "Epoch 236/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3702 - accuracy: 0.8737 - val_loss: 0.3766 - val_accuracy: 0.8776\n",
      "Epoch 237/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3714 - accuracy: 0.8739 - val_loss: 0.4060 - val_accuracy: 0.8693\n",
      "Epoch 238/400\n",
      "781/781 [==============================] - 30s 38ms/step - loss: 0.3650 - accuracy: 0.8735 - val_loss: 0.3961 - val_accuracy: 0.8727\n",
      "Epoch 239/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3685 - accuracy: 0.8735 - val_loss: 0.3941 - val_accuracy: 0.8727\n",
      "Epoch 240/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3686 - accuracy: 0.8728 - val_loss: 0.4043 - val_accuracy: 0.8692\n",
      "Epoch 241/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3662 - accuracy: 0.8745 - val_loss: 0.4240 - val_accuracy: 0.8654\n",
      "Epoch 242/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3690 - accuracy: 0.8728 - val_loss: 0.4269 - val_accuracy: 0.8610\n",
      "Epoch 243/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3670 - accuracy: 0.8739 - val_loss: 0.4022 - val_accuracy: 0.8691\n",
      "Epoch 244/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3668 - accuracy: 0.8760 - val_loss: 0.4009 - val_accuracy: 0.8710\n",
      "Epoch 245/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3681 - accuracy: 0.8741 - val_loss: 0.3856 - val_accuracy: 0.8758\n",
      "Epoch 246/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3623 - accuracy: 0.8755 - val_loss: 0.3897 - val_accuracy: 0.8755\n",
      "Epoch 247/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3650 - accuracy: 0.8757 - val_loss: 0.3901 - val_accuracy: 0.8755\n",
      "Epoch 248/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3607 - accuracy: 0.8752 - val_loss: 0.4029 - val_accuracy: 0.8717\n",
      "Epoch 249/400\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3656 - accuracy: 0.8752 - val_loss: 0.3785 - val_accuracy: 0.8767\n",
      "Epoch 250/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3560 - accuracy: 0.8773 - val_loss: 0.3896 - val_accuracy: 0.8759\n",
      "Epoch 251/400\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3584 - accuracy: 0.8756 - val_loss: 0.4079 - val_accuracy: 0.8693\n",
      "Epoch 252/400\n",
      "451/781 [================>.............] - ETA: 11s - loss: 0.3652 - accuracy: 0.8751"
     ]
    }
   ],
   "source": [
    "# test harness for evaluating models on the cifar10 dataset\n",
    "\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    img_rows, img_cols, channels = 32,32,3\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (img_rows, img_cols, channels), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "  \n",
    "    # compile model\n",
    "    opt = SGD(learning_rate = 0.001, momentum=0.9)\n",
    "    model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    # create data generator\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t  # prepare iterator\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    # fit model \n",
    "    steps = int(trainX.shape[0] / 64)\n",
    "    history = model.fit(it_train, steps_per_epoch = steps, epochs=400, validation_data=(testX, testY), verbose=1)\n",
    "    #history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=1) \n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoKbXgYYDaYY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOWzaDCJUh8e"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "I started by following the hinted material (https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/ ) and that got me to 74.42 accuracy on validation data. The model overfitted around 60 epochs: training accuracy reached 1.000 and validation accuracy got stuck around 70%.\n",
    "\n",
    "I checked out several practical examples of dropout layers. Many of the latter recommended adding them before Dense layers and sticking close to 0.5. Decided to add only one with 0.4 before the only Dense layer I had at this point. This got me over 80% val_accuracy with steady couple percentage points higher training accuracy. The training time was still very fast with Google Colab GPU.\n",
    "\n",
    "Next I followed the hinted material again and added dropout layers. Quite similar results. \n",
    "\n",
    "Adding data augmentation on top of that made the training lot slower (approx. 2 hours). Val_accuracy stayed above 80% after 81 epochs. It kept getting better on avarage, though slowly and ended at 85,35% at 200 epochs. Training accuracy was 87,56%. \n",
    "\n",
    "I added batch normalization, following again the example. My last run ended after 2h 23 min at 252 epochs with loss: 0.3584 - accuracy: 0.8756 - val_loss: 0.4079 - val_accuracy: 0.8693. The model stayed over 80% on val_accuracy after 75 epochs (train_acc 79,61%) and kept very slowly rising from there. The val_accuracy had stayed above 86% after 194 epochs. Colab won't currently offer me more GPU time, so I will hand this in.\n",
    "\n",
    "Even if it took more time, I enjoyed doing this in steps, as it allowed me to get better picture of the effects. With the limited resources anything heavier or trial or learn is hard unless you are able to get good results quickly. I found an interesting tutorial, but that would require even more resources: https://medium.com/fenwicks/tutorial-2-94-accuracy-on-cifar10-in-2-minutes-7b5aaecd9cdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
