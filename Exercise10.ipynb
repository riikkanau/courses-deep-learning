{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10\n",
    "\n",
    "Hyperpameter tuning/optimization: Random and Grid Search\n",
    "\n",
    "Pima Indian Diabetes dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "The datasets consists of several medical predictor variables and one target variable, _Outcome_. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "[Pima Indian Diabes dataset in Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n",
    "\n",
    "Dataset can found from Moodle -> Dataset.\n",
    "\n",
    "NOTE: We use small NN and result will not necessary be very good. This exercise only demonstrate the Grid and Random search methods. \n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Lots of code examples can be found from [this tutorial](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/) for different hyperparameters.\n",
    "\n",
    "### Use scikit-learn to grid search the batch size, epochs, dropout_rate and learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data\\diabetes.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset splitting\n",
    "\n",
    "X = df.drop('Outcome',axis=1).values\n",
    "y = df['Outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.   ,  84.   ,   0.   , ...,   0.   ,   0.304,  21.   ],\n",
       "       [  9.   , 112.   ,  82.   , ...,  28.2  ,   1.282,  50.   ],\n",
       "       [  1.   , 139.   ,  46.   , ...,  28.7  ,   0.654,  22.   ],\n",
       "       ...,\n",
       "       [ 10.   , 101.   ,  86.   , ...,  45.6  ,   1.136,  38.   ],\n",
       "       [  0.   , 141.   ,   0.   , ...,  42.4  ,   0.205,  29.   ],\n",
       "       [  0.   , 125.   ,  96.   , ...,  22.5  ,   0.262,  21.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid search parameters as follows:\n",
    "# learning_rate:[0.001, 0.01, 0.1]\n",
    "# batch_size: [10, 40 , 80]\n",
    "# number of epochs: [10, 50, 100]\n",
    "# dropout_rate:  [0.0, 0.3, 0.5]\n",
    "\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "batch_size = [10, 40, 80]\n",
    "epochs = [10, 50, 100]\n",
    "dropout_rate =  [0.0, 0.3, 0.5]\n",
    "\n",
    "# NOTE: If you don't have enough memory in your computer, drops some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "    #Input layer = 8\n",
    "    #Only one hidden layer = 12 Dense nodes, activation function = relu\n",
    "    #Dropout layer\n",
    "    #Output layer: 1 node, activation = sigmoid\n",
    "\n",
    "def create_model(dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape = (8,), activation = 'relu'))\n",
    "    model.add(Dense(12, activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    opt = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics =['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "#seed = 7\n",
    "#np.random.seed(seed)\n",
    "\n",
    "#Define KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameter dictionary\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, dropout_rate = dropout_rate, learning_rate = learning_rate)\n",
    "\n",
    "#Define grid = GridSearcCV(....)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3,verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.713383 using {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.649872 (0.022117) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.659692 (0.036127) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.648199 (0.014580) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.695449 (0.005582) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.706807 (0.018803) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.688945 (0.044631) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.701857 (0.048509) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.667727 (0.012648) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.657963 (0.011086) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.648262 (0.025442) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.661255 (0.019350) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.646581 (0.017926) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.684019 (0.019968) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.622119 (0.013706) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.644955 (0.008104) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.680815 (0.033101) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.648199 (0.010835) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.659589 (0.023630) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.705237 (0.052903) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 10, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.573258 (0.014921) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.656313 (0.026146) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.683971 (0.034575) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.684059 (0.008499) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.713383 (0.028546) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.654766 (0.023415) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.710075 (0.022238) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.641687 (0.005068) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.649825 (0.016829) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.648199 (0.014580) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.666117 (0.003077) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.685693 (0.037441) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.648175 (0.024725) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.679181 (0.012113) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.620524 (0.019966) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.654719 (0.018029) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.649825 (0.013121) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.654711 (0.006719) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.657979 (0.023910) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.657955 (0.014964) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 40, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.609142 (0.039007) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.573370 (0.054390) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.627092 (0.027274) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.688897 (0.020099) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.648278 (0.030399) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.688793 (0.057551) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.0, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.653037 (0.030781) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.644947 (0.010151) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.644971 (0.025334) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.651451 (0.019090) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.649872 (0.019446) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.674239 (0.014643) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.1}\n",
      "0.595991 (0.043658) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.001}\n",
      "0.636817 (0.003776) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.657979 (0.015951) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.001}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 50, 'learning_rate': 0.1}\n",
      "0.630336 (0.021236) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.001}\n",
      "0.646573 (0.008556) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.01}\n",
      "0.653077 (0.021359) with: {'batch_size': 80, 'dropout_rate': 0.5, 'epochs': 100, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_random2(dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim = 8, activation = 'relu'))\n",
    "    model.add(Dense(12, input_dim = 8, activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    opt = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics =['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#Define KerasClassifier\n",
    "model_random2 = KerasClassifier(build_fn = create_model_random2, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = dict(\n",
    "    learning_rate =  learning_rate, # sp_randFloat(0.001, 0.1), # RuntimeError, if replaced with this / these\n",
    "    batch_size = batch_size,        # sp_randInt(10, 80),\n",
    "    epochs = epochs,                # sp_randInt(10, 100),\n",
    "    dropout_rate = dropout_rate     # sp_randFloat(0.0, 0.5)\n",
    ")\n",
    "\n",
    "#Define random_search = RandomizedSearchCV(...)\n",
    "\n",
    "n_iter_search = 16 # Number of parameter settings that are sampled.\n",
    "random_search2 = RandomizedSearchCV(estimator = model_random2, \n",
    "                                   param_distributions = parameters_dict,\n",
    "                                   n_iter = n_iter_search,\n",
    "                                   n_jobs = 1, # Number of jobs to run in parallel. Default none, meaning in most cases 1.\n",
    "                                   cv = 3 , # default 5-fold cross validation generator\n",
    "                                   verbose = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV 1/3; 1/16] START batch_size=40, dropout_rate=0.5, epochs=50, learning_rate=0.01\n",
      "[CV 1/3; 1/16] END batch_size=40, dropout_rate=0.5, epochs=50, learning_rate=0.01;, score=0.698 total time=   0.9s\n",
      "[CV 2/3; 1/16] START batch_size=40, dropout_rate=0.5, epochs=50, learning_rate=0.01\n",
      "[CV 2/3; 1/16] END batch_size=40, dropout_rate=0.5, epochs=50, learning_rate=0.01;, score=0.634 total time=   0.8s\n",
      "[CV 3/3; 1/16] START batch_size=40, dropout_rate=0.5, epochs=50, learning_rate=0.01\n",
      "[CV 3/3; 1/16] END batch_size=40, dropout_rate=0.5, epochs=50, learning_rate=0.01;, score=0.647 total time=   0.8s\n",
      "[CV 1/3; 2/16] START batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.001\n",
      "[CV 1/3; 2/16] END batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.001;, score=0.683 total time=   1.4s\n",
      "[CV 2/3; 2/16] START batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.001\n",
      "[CV 2/3; 2/16] END batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.001;, score=0.649 total time=   1.1s\n",
      "[CV 3/3; 2/16] START batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.001\n",
      "[CV 3/3; 2/16] END batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.001;, score=0.662 total time=   1.1s\n",
      "[CV 1/3; 3/16] START batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.01\n",
      "[CV 1/3; 3/16] END batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.01;, score=0.707 total time=   0.8s\n",
      "[CV 2/3; 3/16] START batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.01\n",
      "[CV 2/3; 3/16] END batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.01;, score=0.605 total time=   0.8s\n",
      "[CV 3/3; 3/16] START batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.01\n",
      "[CV 3/3; 3/16] END batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.01;, score=0.652 total time=   0.7s\n",
      "[CV 1/3; 4/16] START batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.001\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x0000027621B39B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/3; 4/16] END batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.001;, score=0.673 total time=   1.2s\n",
      "[CV 2/3; 4/16] START batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.001\n",
      "[CV 2/3; 4/16] END batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.001;, score=0.654 total time=   1.1s\n",
      "[CV 3/3; 4/16] START batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.001\n",
      "[CV 3/3; 4/16] END batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.001;, score=0.686 total time=   1.1s\n",
      "[CV 1/3; 5/16] START batch_size=10, dropout_rate=0.3, epochs=100, learning_rate=0.01\n",
      "[CV 1/3; 5/16] END batch_size=10, dropout_rate=0.3, epochs=100, learning_rate=0.01;, score=0.746 total time=   2.9s\n",
      "[CV 2/3; 5/16] START batch_size=10, dropout_rate=0.3, epochs=100, learning_rate=0.01\n",
      "[CV 2/3; 5/16] END batch_size=10, dropout_rate=0.3, epochs=100, learning_rate=0.01;, score=0.659 total time=   3.0s\n",
      "[CV 3/3; 5/16] START batch_size=10, dropout_rate=0.3, epochs=100, learning_rate=0.01\n",
      "[CV 3/3; 5/16] END batch_size=10, dropout_rate=0.3, epochs=100, learning_rate=0.01;, score=0.745 total time=   2.5s\n",
      "[CV 1/3; 6/16] START batch_size=10, dropout_rate=0.3, epochs=10, learning_rate=0.001\n",
      "[CV 1/3; 6/16] END batch_size=10, dropout_rate=0.3, epochs=10, learning_rate=0.001;, score=0.654 total time=   0.7s\n",
      "[CV 2/3; 6/16] START batch_size=10, dropout_rate=0.3, epochs=10, learning_rate=0.001\n",
      "[CV 2/3; 6/16] END batch_size=10, dropout_rate=0.3, epochs=10, learning_rate=0.001;, score=0.615 total time=   0.8s\n",
      "[CV 3/3; 6/16] START batch_size=10, dropout_rate=0.3, epochs=10, learning_rate=0.001\n",
      "[CV 3/3; 6/16] END batch_size=10, dropout_rate=0.3, epochs=10, learning_rate=0.001;, score=0.632 total time=   0.9s\n",
      "[CV 1/3; 7/16] START batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.1\n",
      "[CV 1/3; 7/16] END batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.1;, score=0.683 total time=   0.7s\n",
      "[CV 2/3; 7/16] START batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.1\n",
      "[CV 2/3; 7/16] END batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.1;, score=0.634 total time=   0.7s\n",
      "[CV 3/3; 7/16] START batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.1\n",
      "[CV 3/3; 7/16] END batch_size=80, dropout_rate=0.0, epochs=50, learning_rate=0.1;, score=0.662 total time=   0.8s\n",
      "[CV 1/3; 8/16] START batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.1\n",
      "WARNING:tensorflow:5 out of the last 31 calls to <function Model.make_test_function.<locals>.test_function at 0x00000276219AC820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/3; 8/16] END batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.1;, score=0.683 total time=   1.1s\n",
      "[CV 2/3; 8/16] START batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.1\n",
      "[CV 2/3; 8/16] END batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.1;, score=0.634 total time=   1.2s\n",
      "[CV 3/3; 8/16] START batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.1\n",
      "[CV 3/3; 8/16] END batch_size=40, dropout_rate=0.0, epochs=100, learning_rate=0.1;, score=0.642 total time=   1.4s\n",
      "[CV 1/3; 9/16] START batch_size=40, dropout_rate=0.5, epochs=10, learning_rate=0.1\n",
      "[CV 1/3; 9/16] END batch_size=40, dropout_rate=0.5, epochs=10, learning_rate=0.1;, score=0.683 total time=   0.7s\n",
      "[CV 2/3; 9/16] START batch_size=40, dropout_rate=0.5, epochs=10, learning_rate=0.1\n",
      "[CV 2/3; 9/16] END batch_size=40, dropout_rate=0.5, epochs=10, learning_rate=0.1;, score=0.634 total time=   0.5s\n",
      "[CV 3/3; 9/16] START batch_size=40, dropout_rate=0.5, epochs=10, learning_rate=0.1\n",
      "[CV 3/3; 9/16] END batch_size=40, dropout_rate=0.5, epochs=10, learning_rate=0.1;, score=0.642 total time=   0.5s\n",
      "[CV 1/3; 10/16] START batch_size=80, dropout_rate=0.0, epochs=100, learning_rate=0.1\n",
      "[CV 1/3; 10/16] END batch_size=80, dropout_rate=0.0, epochs=100, learning_rate=0.1;, score=0.761 total time=   1.1s\n",
      "[CV 2/3; 10/16] START batch_size=80, dropout_rate=0.0, epochs=100, learning_rate=0.1\n",
      "[CV 2/3; 10/16] END batch_size=80, dropout_rate=0.0, epochs=100, learning_rate=0.1;, score=0.634 total time=   0.8s\n",
      "[CV 3/3; 10/16] START batch_size=80, dropout_rate=0.0, epochs=100, learning_rate=0.1\n",
      "[CV 3/3; 10/16] END batch_size=80, dropout_rate=0.0, epochs=100, learning_rate=0.1;, score=0.642 total time=   0.9s\n",
      "[CV 1/3; 11/16] START batch_size=10, dropout_rate=0.0, epochs=50, learning_rate=0.1\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x000002762498F550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 11/16] END batch_size=10, dropout_rate=0.0, epochs=50, learning_rate=0.1;, score=0.683 total time=   1.6s\n",
      "[CV 2/3; 11/16] START batch_size=10, dropout_rate=0.0, epochs=50, learning_rate=0.1\n",
      "[CV 2/3; 11/16] END batch_size=10, dropout_rate=0.0, epochs=50, learning_rate=0.1;, score=0.634 total time=   1.7s\n",
      "[CV 3/3; 11/16] START batch_size=10, dropout_rate=0.0, epochs=50, learning_rate=0.1\n",
      "[CV 3/3; 11/16] END batch_size=10, dropout_rate=0.0, epochs=50, learning_rate=0.1;, score=0.642 total time=   1.6s\n",
      "[CV 1/3; 12/16] START batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.01\n",
      "[CV 1/3; 12/16] END batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.01;, score=0.678 total time=   1.1s\n",
      "[CV 2/3; 12/16] START batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.01\n",
      "[CV 2/3; 12/16] END batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.01;, score=0.634 total time=   1.1s\n",
      "[CV 3/3; 12/16] START batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.01\n",
      "[CV 3/3; 12/16] END batch_size=40, dropout_rate=0.3, epochs=100, learning_rate=0.01;, score=0.627 total time=   1.4s\n",
      "[CV 1/3; 13/16] START batch_size=10, dropout_rate=0.3, epochs=50, learning_rate=0.1\n",
      "[CV 1/3; 13/16] END batch_size=10, dropout_rate=0.3, epochs=50, learning_rate=0.1;, score=0.683 total time=   1.6s\n",
      "[CV 2/3; 13/16] START batch_size=10, dropout_rate=0.3, epochs=50, learning_rate=0.1\n",
      "[CV 2/3; 13/16] END batch_size=10, dropout_rate=0.3, epochs=50, learning_rate=0.1;, score=0.634 total time=   1.5s\n",
      "[CV 3/3; 13/16] START batch_size=10, dropout_rate=0.3, epochs=50, learning_rate=0.1\n",
      "[CV 3/3; 13/16] END batch_size=10, dropout_rate=0.3, epochs=50, learning_rate=0.1;, score=0.642 total time=   1.4s\n",
      "[CV 1/3; 14/16] START batch_size=40, dropout_rate=0.3, epochs=10, learning_rate=0.1\n",
      "[CV 1/3; 14/16] END batch_size=40, dropout_rate=0.3, epochs=10, learning_rate=0.1;, score=0.683 total time=   0.7s\n",
      "[CV 2/3; 14/16] START batch_size=40, dropout_rate=0.3, epochs=10, learning_rate=0.1\n",
      "[CV 2/3; 14/16] END batch_size=40, dropout_rate=0.3, epochs=10, learning_rate=0.1;, score=0.634 total time=   0.5s\n",
      "[CV 3/3; 14/16] START batch_size=40, dropout_rate=0.3, epochs=10, learning_rate=0.1\n",
      "[CV 3/3; 14/16] END batch_size=40, dropout_rate=0.3, epochs=10, learning_rate=0.1;, score=0.642 total time=   0.5s\n",
      "[CV 1/3; 15/16] START batch_size=40, dropout_rate=0.3, epochs=50, learning_rate=0.1\n",
      "[CV 1/3; 15/16] END batch_size=40, dropout_rate=0.3, epochs=50, learning_rate=0.1;, score=0.683 total time=   0.8s\n",
      "[CV 2/3; 15/16] START batch_size=40, dropout_rate=0.3, epochs=50, learning_rate=0.1\n",
      "[CV 2/3; 15/16] END batch_size=40, dropout_rate=0.3, epochs=50, learning_rate=0.1;, score=0.634 total time=   0.9s\n",
      "[CV 3/3; 15/16] START batch_size=40, dropout_rate=0.3, epochs=50, learning_rate=0.1\n",
      "[CV 3/3; 15/16] END batch_size=40, dropout_rate=0.3, epochs=50, learning_rate=0.1;, score=0.642 total time=   0.8s\n",
      "[CV 1/3; 16/16] START batch_size=40, dropout_rate=0.0, epochs=10, learning_rate=0.001\n",
      "[CV 1/3; 16/16] END batch_size=40, dropout_rate=0.0, epochs=10, learning_rate=0.001;, score=0.517 total time=   0.6s\n",
      "[CV 2/3; 16/16] START batch_size=40, dropout_rate=0.0, epochs=10, learning_rate=0.001\n",
      "[CV 2/3; 16/16] END batch_size=40, dropout_rate=0.0, epochs=10, learning_rate=0.001;, score=0.605 total time=   0.8s\n",
      "[CV 3/3; 16/16] START batch_size=40, dropout_rate=0.0, epochs=10, learning_rate=0.001\n",
      "[CV 3/3; 16/16] END batch_size=40, dropout_rate=0.0, epochs=10, learning_rate=0.001;, score=0.632 total time=   0.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000002761F4353D0>,\n",
       "                   n_iter=16, n_jobs=1,\n",
       "                   param_distributions={'batch_size': [10, 40, 80],\n",
       "                                        'dropout_rate': [0.0, 0.3, 0.5],\n",
       "                                        'epochs': [10, 50, 100],\n",
       "                                        'learning_rate': [0.001, 0.01, 0.1]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.716659 using {'learning_rate': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 10}\n",
      "0.659589 (0.027363) with: {'learning_rate': 0.01, 'epochs': 50, 'dropout_rate': 0.5, 'batch_size': 40}\n",
      "0.664491 (0.014073) with: {'learning_rate': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 40}\n",
      "0.654719 (0.041866) with: {'learning_rate': 0.01, 'epochs': 50, 'dropout_rate': 0.0, 'batch_size': 80}\n",
      "0.671035 (0.013401) with: {'learning_rate': 0.001, 'epochs': 100, 'dropout_rate': 0.0, 'batch_size': 40}\n",
      "0.716659 (0.041102) with: {'learning_rate': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 10}\n",
      "0.633549 (0.015954) with: {'learning_rate': 0.001, 'epochs': 10, 'dropout_rate': 0.3, 'batch_size': 10}\n",
      "0.659613 (0.019973) with: {'learning_rate': 0.1, 'epochs': 50, 'dropout_rate': 0.0, 'batch_size': 80}\n",
      "0.653077 (0.021359) with: {'learning_rate': 0.1, 'epochs': 100, 'dropout_rate': 0.0, 'batch_size': 40}\n",
      "0.653077 (0.021359) with: {'learning_rate': 0.1, 'epochs': 10, 'dropout_rate': 0.5, 'batch_size': 40}\n",
      "0.679093 (0.057992) with: {'learning_rate': 0.1, 'epochs': 100, 'dropout_rate': 0.0, 'batch_size': 80}\n",
      "0.653077 (0.021359) with: {'learning_rate': 0.1, 'epochs': 50, 'dropout_rate': 0.0, 'batch_size': 10}\n",
      "0.646549 (0.022441) with: {'learning_rate': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 40}\n",
      "0.653077 (0.021359) with: {'learning_rate': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 10}\n",
      "0.653077 (0.021359) with: {'learning_rate': 0.1, 'epochs': 10, 'dropout_rate': 0.3, 'batch_size': 40}\n",
      "0.653077 (0.021359) with: {'learning_rate': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'batch_size': 40}\n",
      "0.584768 (0.049164) with: {'learning_rate': 0.001, 'epochs': 10, 'dropout_rate': 0.0, 'batch_size': 40}\n"
     ]
    }
   ],
   "source": [
    "#Summarize results\n",
    "print(\"Best: %f using %s\" % (random_search2.best_score_, random_search2.best_params_))\n",
    "means = random_search2.cv_results_['mean_test_score']\n",
    "stds = random_search2.cv_results_['std_test_score']\n",
    "params = random_search2.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "GridSearch:\n",
    "Best: 0.713383 using {'batch_size': 40, 'dropout_rate': 0.0, 'epochs': 50, 'learning_rate': 0.01}\n",
    "RandomSearchCV:\n",
    "Best: 0.716659 using {'learning_rate': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'batch_size': 10}\n",
    "\n",
    "If I try to generate random parameters when using RandomSearchCV, the fit will end at cannot clone object RuntimeError that seems to be a known issue after scikit 0.21.2. So, I ended up using set values and I am very interested in getting help in applying the randomness of the random search :)\n",
    "\n",
    "This application of random searh seemed to be bit faster than grid search. Both are a lot handier in hyperparameter tuning than guesswork.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search - aiempi yritys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create model\n",
    "# #Same as in Grid search example \n",
    "\n",
    "# def create_model_random(dropout_rate, learning_rate):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(8, input_shape = (8,), activation = 'relu'))\n",
    "#     model.add(Dense(12, activation = 'relu'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "#     opt = Adam(learning_rate = learning_rate)\n",
    "#     model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics =['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# #Define KerasClassifier\n",
    "# model_random = KerasClassifier(build_fn = create_model_random, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the Random search parameters\n",
    "# # randomly from same range as the Grid search\n",
    "\n",
    "# parameters = {\n",
    "#     'learning_rate': sp_randFloat(0.001, 0.1), # uniform\n",
    "#     'batch_size': sp_randInt(10, 80), # randint\n",
    "#     'epochs': sp_randInt(10, 100), # randint\n",
    "#     'dropout_rate': sp_randFloat(0.0, 0.5) # uniform\n",
    "# }\n",
    "\n",
    "\n",
    "# #Define random_search = RandomizedSearchCV(...)\n",
    "\n",
    "# n_iter_search = 16 # Number of parameter settings that are sampled.\n",
    "# random_search = RandomizedSearchCV(estimator = model_random, \n",
    "#                                    param_distributions = parameters,\n",
    "#                                    n_iter = n_iter_search,\n",
    "#                                    #n_jobs = 1, # Number of jobs to run in parallel. Default none, meaning in most cases 1.\n",
    "#                                    #cv = 2 , # default 5-fold cross validation generator\n",
    "#                                    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Results\n",
    "# random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Summarize results\n",
    "# print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "# means = random_search.cv_results_['mean_test_score']\n",
    "# stds = random_search.cv_results_['std_test_score']\n",
    "# params = random_search.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
